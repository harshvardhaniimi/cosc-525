{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 13:47:26.068279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Keras layers\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Lambda, Reshape, Conv2DTranspose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_age(age):\n",
    "    age_dict = {'0-2': 0, '3-9': 1, '10-19': 2, '20-29': 3, '30-39': 4, '40-49': 5, '50-59': 6, '60-69': 7, 'more than 70': 8}\n",
    "    return np.eye(9)[[age_dict[a] for a in age]]\n",
    "\n",
    "def convert_gender(gender):\n",
    "    gender_dict = {'Male': 0, 'Female': 1}\n",
    "    return np.eye(2)[[gender_dict[g] for g in gender]]\n",
    "\n",
    "def convert_race(race):\n",
    "    race_dict = {'Black': 0, 'Latino_Hispanic': 1, 'East Asian': 2, 'White': 3, 'Southeast Asian': 4, 'Middle Eastern': 5, 'Indian': 6}\n",
    "    return np.eye(7)[[race_dict[r] for r in race]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading FairFace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read image\n",
    "def read_image(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        return np.asarray(image)\n",
    "\n",
    "# function to read image\n",
    "def read_image(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        return np.asarray(image)\n",
    "\n",
    "# read data, labels in lists\n",
    "def get_dataset(DATA_DIR, mode, sample=False):\n",
    "    if mode == 'train':\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, 'fairface_label_train.csv'))\n",
    "    elif mode == 'val':\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, 'fairface_label_val.csv'))\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    age = df['age'].values.tolist()\n",
    "    gender = df['gender'].values.tolist()\n",
    "    race = df['race'].values.tolist()\n",
    "    filenames = df['file'].values.tolist()\n",
    "\n",
    "    image_paths = [os.path.join(DATA_DIR, name) for name in filenames]\n",
    "\n",
    "    if sample:\n",
    "        if mode == 'train':\n",
    "            sample_size = 6000\n",
    "        elif mode == 'val':\n",
    "            sample_size = 1000\n",
    "        sampled_indexes = random.sample(range(len(image_paths)), sample_size)\n",
    "        image_paths = [image_paths[i] for i in sampled_indexes]\n",
    "        age = [age[i] for i in sampled_indexes]\n",
    "        gender = [gender[i] for i in sampled_indexes]\n",
    "        race = [race[i] for i in sampled_indexes]\n",
    "\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        all_img = list(executor.map(read_image, image_paths))\n",
    "    \n",
    "    onehot_age = convert_age(age)\n",
    "    onehot_gender = convert_gender(gender)\n",
    "    onehot_race = convert_race(race)\n",
    "\n",
    "    return all_img, onehot_age, onehot_gender, onehot_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/harshvardhan/Library/CloudStorage/Dropbox/Academics/UTK Classes/Spring 2023/Deep Learning/Final Project - FairFace Data/cosc-525-final-project/fairface-img-margin025-trainval/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, train_age, train_gender, train_race = get_dataset(DATA_DIR, 'train', sample=True)\n",
    "val_img, val_age, val_gender, val_race = get_dataset(DATA_DIR, 'val', sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"/Users/harshvardhan/Library/CloudStorage/Dropbox/Academics/UTK Classes/Spring 2023/Deep Learning/Final Project - FairFace Data/cosc-525-final-project/fairface-img-margin025-trainval/train/1.jpg\")\n",
    "print(img.size)  # prints the width and height of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scaling\n",
    "flattened_train_img = [img.reshape(224*224*3) for img in train_img]\n",
    "flattened_val_img = [img.reshape(224*224*3) for img in val_img]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(flattened_train_img)\n",
    "\n",
    "scaled_train_img = scaler.transform(flattened_train_img)\n",
    "scaled_val_img = scaler.transform(flattened_val_img)\n",
    "\n",
    "scaled_train_img = np.array([img.reshape(224,224,3) for img in scaled_train_img])\n",
    "scaled_val_img = np.array([img.reshape(224,224,3) for img in scaled_val_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scaled_val_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    #Extract mean and log of variance\n",
    "    z_mean, z_log_var = args\n",
    "    #get batch size and length of vector (size of latent space)\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    #Return sampled number (need to raise var to correct power)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 16:14:10.362918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(224, 224, 3), name='encoder_input')\n",
    "x = Conv2D(filters=16, kernel_size=3, strides=(1, 1), padding=\"valid\", activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
    "x = Conv2D(filters=32, kernel_size=3, strides=(1, 1), padding='valid', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_output\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 222, 222, 16  448         ['encoder_input[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 111, 111, 16  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 109, 109, 32  4640        ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 54, 54, 32)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 52, 52, 64)   18496       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 26, 26, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 43264)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          5537920     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 10)           1290        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 10)           1290        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 10)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,564,084\n",
      "Trainable params: 5,564,084\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "z = Lambda(sampling, name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, z, name='encoder_output')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "\n",
    "x = Dense(7*7*512, activation='relu', name=\"decoder_hidden_layer\")(latent_inputs)\n",
    "x = Reshape((7,7,512))(x)\n",
    "x = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = 2, padding = 'same',activation = 'relu')(x)\n",
    "x = Conv2DTranspose(filters=128,kernel_size=(4,4),strides=2,padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(filters=64,kernel_size=(4,4),strides=2,padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(filters=32,kernel_size=(4,4),strides=2,padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(filters=3,kernel_size=(4,4),strides=2,padding='same', activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_output\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 10)]              0         \n",
      "                                                                 \n",
      " decoder_hidden_layer (Dense  (None, 25088)            275968    \n",
      " )                                                               \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 256)      2097408   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 128)      524416    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 56, 56, 64)       131136    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 112, 112, 32)     32800     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 224, 224, 3)      1539      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,063,267\n",
      "Trainable params: 3,063,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs=x, name='decoder_output')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " encoder_output (Functional)  (None, 10)               5564084   \n",
      "                                                                 \n",
      " decoder_output (Functional)  (None, 224, 224, 3)      3063267   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,627,351\n",
      "Trainable params: 8,627,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "outputs = decoder(encoder(inputs))\n",
    "vae = Model(inputs = inputs, outputs = outputs)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "# setting loss\n",
    "reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "reconstruction_loss *= (32*32) #image_width*image_height\n",
    "\n",
    "kl_loss = K.exp(z_log_var) + K.square(z_mean) - z_log_var - 1\n",
    "print(kl_loss.shape)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= 0.05\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 265s 2s/step - loss: 37.3279\n"
     ]
    }
   ],
   "source": [
    "train_history = vae.fit(scaled_train_img, scaled_train_img, batch_size=bs, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 89ms/step\n"
     ]
    }
   ],
   "source": [
    "encoded_val_images = encoder.predict(scaled_val_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_val_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base shared layers\n",
    "base_input = Input(shape=(latent_dim,), name='base_input')\n",
    "x = Dense(7*7*64, activation='relu')(base_input)\n",
    "x = Reshape((7, 7, 64))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common CNN layers\n",
    "x = Conv2D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Age output\n",
    "age_output = Dense(64, activation='relu')(x)\n",
    "age_output = Dense(9, activation='softmax', name='age_output')(age_output)\n",
    "\n",
    "# Gender output\n",
    "gender_output = Dense(64, activation='relu')(x)\n",
    "gender_output = Dense(2, activation='softmax', name='gender_output')(gender_output)\n",
    "\n",
    "# Race output\n",
    "race_output = Dense(64, activation='relu')(x)\n",
    "race_output = Dense(7, activation='softmax', name='race_output')(race_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine into a single model\n",
    "classifier_model = Model(inputs=base_input, outputs=[age_output, gender_output, race_output])\n",
    "\n",
    "# Compile the classifier model with appropriate loss functions and metrics\n",
    "classifier_model.compile(optimizer='adam',\n",
    "                         loss={'age_output': 'categorical_crossentropy',\n",
    "                               'gender_output': 'categorical_crossentropy',\n",
    "                               'race_output': 'categorical_crossentropy'},\n",
    "                         metrics={'age_output': 'accuracy',\n",
    "                                  'gender_output': 'accuracy',\n",
    "                                  'race_output': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " base_input (InputLayer)        [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3136)         34496       ['base_input[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 7, 7, 64)     0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 7, 7, 128)    73856       ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 128)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 3, 3, 64)     73792       ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 64)    0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 64)           0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           4160        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           4160        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           4160        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " age_output (Dense)             (None, 9)            585         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " gender_output (Dense)          (None, 2)            130         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " race_output (Dense)            (None, 7)            455         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 195,794\n",
      "Trainable params: 195,794\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 15s 105ms/step\n"
     ]
    }
   ],
   "source": [
    "encoded_train_images = encoder.predict(scaled_train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 2s 42ms/step - loss: 4.6086 - age_output_loss: 1.9748 - gender_output_loss: 0.6965 - race_output_loss: 1.9373 - age_output_accuracy: 0.2475 - gender_output_accuracy: 0.5070 - race_output_accuracy: 0.1922 - val_loss: 4.5223 - val_age_output_loss: 1.8787 - val_gender_output_loss: 0.7085 - val_race_output_loss: 1.9351 - val_age_output_accuracy: 0.3134 - val_gender_output_accuracy: 0.4401 - val_race_output_accuracy: 0.2074\n"
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(encoded_train_images,\n",
    "                               {'age_output': train_age,\n",
    "                                'gender_output': train_gender,\n",
    "                                'race_output': train_race},\n",
    "                               epochs=1,  # adjust this value based on desired training time and performance\n",
    "                               batch_size=128,  # adjust this value based on your hardware capabilities\n",
    "                               validation_split=0.1)  # 10% of the training data will be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scaled_val_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 98ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 547\n  y sizes: 10000, 10000, 10000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m encoded_val_images \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mpredict(scaled_val_img)\n\u001b[0;32m----> 3\u001b[0m results \u001b[39m=\u001b[39m classifier_model\u001b[39m.\u001b[39;49mevaluate(encoded_val_images,\n\u001b[1;32m      4\u001b[0m                                     {\u001b[39m'\u001b[39;49m\u001b[39mage_output\u001b[39;49m\u001b[39m'\u001b[39;49m: val_age,\n\u001b[1;32m      5\u001b[0m                                      \u001b[39m'\u001b[39;49m\u001b[39mgender_output\u001b[39;49m\u001b[39m'\u001b[39;49m: val_gender,\n\u001b[1;32m      6\u001b[0m                                      \u001b[39m'\u001b[39;49m\u001b[39mrace_output\u001b[39;49m\u001b[39m'\u001b[39;49m: val_race},\n\u001b[1;32m      7\u001b[0m                                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1845\u001b[0m         label,\n\u001b[1;32m   1846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m   1847\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1848\u001b[0m         ),\n\u001b[1;32m   1849\u001b[0m     )\n\u001b[1;32m   1850\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1851\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 547\n  y sizes: 10000, 10000, 10000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "encoded_val_images = encoder.predict(scaled_val_img)\n",
    "\n",
    "results = classifier_model.evaluate(encoded_val_images,\n",
    "                                    {'age_output': val_age,\n",
    "                                     'gender_output': val_gender,\n",
    "                                     'race_output': val_race},\n",
    "                                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

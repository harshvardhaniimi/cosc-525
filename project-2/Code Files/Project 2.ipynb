{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Project 2\"\n",
        "author: \"Harshvardhan and Yu Jiang\"\n",
        "description: \"COSC 525: Deep Learning / Spring 2023\"\n",
        "format: html\n",
        "theme: cosmo\n",
        "self-contained: true\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LqTHleh4tWSe"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "azNuEUqlqJvx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-06 15:17:57.978741: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# For this project, stride is always 1 and padding is set to valid.\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrIZHZ7ftm9Y"
      },
      "source": [
        "# Neuron Convolution Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f6knYnUwsKrW"
      },
      "outputs": [],
      "source": [
        "#Only for the neurons in convolutional layer\n",
        "class NeuronConv:\n",
        "    def __init__(self, activation, kernelSize, lr, weights,bias=None):\n",
        "        self.output = None\n",
        "        self.input = None\n",
        "        self.delta = None\n",
        "\n",
        "        self.activation = activation\n",
        "        self.kernelSize = kernelSize\n",
        "        self.lr = lr\n",
        "        self.weights = weights # 3D in the right shape\n",
        "        self.bias = 0 if bias is None else bias# scalar: a single value\n",
        "\n",
        "\n",
        "\n",
        "    # This method returns the activation of the net\n",
        "    # Since the activation functions in all convolution layers are sigmoid in this project, here we only provide one way.\n",
        "    def activate(self,activation,net):\n",
        "        if activation == 'sigmoid':\n",
        "            output = 1 / (1 + np.exp(-1 * net))\n",
        "        else:\n",
        "            output = net\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    # Calculate the output of the neuron should save the input and output for back-propagation.\n",
        "    def calculate(self,input): #input should have the same size as kernel. (channels,H,W)\n",
        "\n",
        "        dimension =np.shape(self.weights)\n",
        "        channels = dimension[0]\n",
        "\n",
        "        #create an empty matrix to contain the internal steps\n",
        "        matrix = np.zeros(self.weights.size).reshape(dimension)\n",
        "\n",
        "        for c in range(channels): #channels\n",
        "            for i in range(self.kernelSize): # Row\n",
        "                for j in range(self.kernelSize): #Column\n",
        "                    matrix[c,i,j] = input[c,i,j] * self.weights[c,i,j]\n",
        "        output = np.sum(matrix)+self.bias\n",
        "\n",
        "        self.output=self.activate(self.activation,output)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    # This method returns the derivative of the activation function with respect to the net\n",
        "    def activationderivative(self):\n",
        "        if self.activation =='sigmoid':\n",
        "            act_der = self.output*(1-self.output)\n",
        "        else: #linear\n",
        "            act_der=1\n",
        "        return act_der\n",
        "\n",
        "    def calcpartialderivative(self, wtimesdelta):  # wtimesdelta is a matrix\n",
        "        self.delta = wtimesdelta * self.activationderivative()\n",
        "        return self.delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndE52zVhtqNL"
      },
      "source": [
        "# Convolution Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7RsFYfJoseg4"
      },
      "outputs": [],
      "source": [
        "# A Convolutional layer\n",
        "class ConvolutionalLayer:\n",
        "\n",
        "    # initialize with the number of neurons in the layer, their activation,the input size,the learning rate\n",
        "    # inputDimension (C,H,W)  3D matrix\n",
        "    # bias will be a 2D matrix (C, 1)\n",
        "    # weight should be 4D (fliter sequence, c,H,W)\n",
        "    def __init__(self, numOfKernels, kernelSize, activation, inputDimension,lr, weights, bias=None):\n",
        "\n",
        "        self.inputDimension = inputDimension\n",
        "        self.kernelSize = kernelSize\n",
        "        self.weights = np.random.random((numOfKernels,self.inputDimension[0],kernelSize, kernelSize)) if weights is None else weights.reshape(numOfKernels,self.inputDimension[0],kernelSize, kernelSize)\n",
        "        self.bias = np.zeros((numOfKernels)) if bias is None else bias\n",
        "        self.lr = lr\n",
        "\n",
        "        #the output dimension = (channels,height, width)\n",
        "        # Although in this project width is equal to heights, we can still write them separately for future more generalized application;\n",
        "        # channels = numOfKernels\n",
        "        # width = (width_input-width_kernel)/stride +1\n",
        "        self.width = int((inputDimension[2]-kernelSize)/1+1)\n",
        "        self.height = int((inputDimension[1]-kernelSize)/1+1)\n",
        "        self.channels = numOfKernels # output channels != input channels\n",
        "        self.numOfOutputNeuron = int(self.width*self.height*self.channels) #total # of output neurons\n",
        "        self.numOfOutputNeuron_OneLayer = int(self.width*self.height)#  # of output neurons in one feature map\n",
        "\n",
        "        self.outputDimension =np.array((self.channels,self.height,self.width)) #Will be the dimension of input of next layer\n",
        "\n",
        "\n",
        "        # Store the output of this layer, matrix )\n",
        "        self.output = np.zeros(self.numOfOutputNeuron).reshape((self.channels, self.height,self.width))\n",
        "\n",
        "        # create the output layer\n",
        "        self.neurons= [] # list of list [[output of filter 1],[Output of filter 2],...]\n",
        "\n",
        "\n",
        "        for i in range(self.channels): # equal to the filter number\n",
        "            neurons= []\n",
        "            for j in range(self.numOfOutputNeuron_OneLayer):\n",
        "                neurons.append(NeuronConv(activation,kernelSize,lr, self.weights[i], self.bias[i]))\n",
        "            self.neurons.append(neurons)\n",
        "\n",
        "    def get_outputDimension(self):\n",
        "        return self.outputDimension\n",
        "\n",
        "    def calculate(self,input): # input is 3D ( channelsOfInput,widthOfInput, heightOfInput)\n",
        "        self.input = input\n",
        "        total_output =[]\n",
        "        # Separate the entire input into a list of input for each output neuron\n",
        "        input_lst = []\n",
        "        for i in range(self.height): #get the data of each row\n",
        "            for j in range(self.width): #column\n",
        "                input_lst.append(self.input[:, i:(i+self.kernelSize),j:(j+self.kernelSize)])\n",
        "\n",
        "        for i in range(self.channels):\n",
        "            output_layer=[]\n",
        "            for j in range(self.numOfOutputNeuron_OneLayer):\n",
        "                output_layer.append(self.neurons[i][j].calculate(input_lst[j]))\n",
        "            output_matrix_onelayer = np.array(output_layer).reshape((self.height, self.width))\n",
        "            total_output.append(output_matrix_onelayer)\n",
        "\n",
        "        self.output = np.array(total_output)\n",
        "        return self.output\n",
        "\n",
        "    # This method calculates the partial derivative for each weight and returns the delta*w to be used in the previous layer\n",
        "    # Compared with the fully connected neuron, the update of the learning parameter occurs at the ConvolutionalLayer\n",
        "    def calcwdeltas(self, wtimesdelta):  # wtimesdelta is a matrix\n",
        "        #Same logic as calculate the output\n",
        "        total_delta=[]\n",
        "\n",
        "        for i in range(self.channels):# Num of kernel\n",
        "            delta_layer =[]\n",
        "            for j in range(self.numOfOutputNeuron_OneLayer):\n",
        "\n",
        "                delta_layer.append(self.neurons[i][j].calcpartialderivative(wtimesdelta[i,j//self.width,j%self.width]))\n",
        "\n",
        "            delta_layer = np.array(delta_layer).reshape((self.height,self.width))\n",
        "\n",
        "            total_delta.append(delta_layer)\n",
        "        self.delta = np.array(total_delta)\n",
        "\n",
        "        #Add padding to delta Since padding is 'same'.\n",
        "        # (new-kernealsize)/stride +1 = initial\n",
        "        # new = (initial-1)*stride + kernelSize\n",
        "        new_width = self.inputDimension[2]-1+self.kernelSize\n",
        "        new_height = self.inputDimension[1]-1+self.kernelSize\n",
        "\n",
        "        # flipped the weights of each layer per filter\n",
        "        new_weights = np.zeros(self.weights.size).reshape(np.shape(self.weights))\n",
        "        for i in range(np.shape(self.weights)[0]): #different filters\n",
        "            for j in range(np.shape(self.weights)[1]): #different channels corresponding to the input channels\n",
        "                new_weights[i,j]= self.weights[i,j].T\n",
        "\n",
        "\n",
        "        lst2=[]\n",
        "        for i in range(self.inputDimension[0]):\n",
        "            lst = []\n",
        "            for j in range(np.shape(self.delta)[0]):\n",
        "                new_padding_matrix = np.zeros((1,new_height,new_width))\n",
        "                new_padding_matrix[0,self.kernelSize-1:(self.kernelSize-1+self.height),self.kernelSize-1:(self.kernelSize-1+self.width) ] = self.delta[j,:,:]\n",
        "                lst.append(ConvolutionalLayer(1, self.kernelSize, 'linear', np.shape(new_padding_matrix), self.lr,\n",
        "                               np.expand_dims(new_weights[j,i],axis=0), bias=None).calculate(new_padding_matrix))\n",
        "            lst = np.array(lst)\n",
        "\n",
        "            lst2.append(np.sum(lst, axis=0).reshape(self.inputDimension[1],self.inputDimension[2]))\n",
        "        lst2= np.array(lst2)\n",
        "        wtimesdelta = lst2\n",
        "\n",
        "        #update weights\n",
        "        self.updateweight()\n",
        "\n",
        "        return wtimesdelta\n",
        "\n",
        "\n",
        "    #numOfKernels, kernelSize, activation, inputDimension, lr, weights, bias):\n",
        "    def updateweight(self):\n",
        "        total_deltas=[]#4D #of filters, channels\n",
        "\n",
        "        for i in range(self.delta.shape[0]):\n",
        "            new_deltas = []\n",
        "            for j in range(self.inputDimension[0]):\n",
        "                new_deltas.append(self.delta[i,:,:])\n",
        "            new_deltas = np.array(new_deltas)\n",
        "            total_deltas.append(new_deltas)\n",
        "        total_deltas =np.array(total_deltas)\n",
        "\n",
        "        for i in range(self.delta.shape[0]):\n",
        "            self.weights[i] -= self.lr * ConvolutionalLayer(1, self.delta.shape[-1],'linear', self.inputDimension, self.lr, np.expand_dims(total_deltas[i],axis=0), None).calculate(self.input)\n",
        "\n",
        "        self.bias -= self.lr * np.sum(self.delta)\n",
        "\n",
        "    def print_weights(self,layer_idx):\n",
        "        print('layer {} is a Conv layer, weights shape = {} and bias shape = {}. Weights and bias are as follows:'.format(layer_idx, self.weights.shape, self.bias.shape))\n",
        "        print(self.weights)\n",
        "        print(self.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLAQRBo2ttn7"
      },
      "source": [
        "# MaxPoolingLayer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5C5KSjY_uJyA"
      },
      "outputs": [],
      "source": [
        "class MaxPoolingLayer:\n",
        "    def __init__(self, kernelSize, inputDimension): #inputDimension (channels, height, width)\n",
        "\n",
        "        self.kernelSize = kernelSize\n",
        "        self.inputDimension = np.array(inputDimension)\n",
        "        self.initial_input_number = np.prod(inputDimension)\n",
        "        # For this project, stride is always the same as the filter size; no padding.\n",
        "        self.height = int((inputDimension[1]-kernelSize)/kernelSize + 1)\n",
        "        self.width = int((inputDimension[2]-kernelSize)/kernelSize + 1)\n",
        "        self.channels =int(inputDimension[0])\n",
        "        total_neuron = int(self.channels * self.height * self.width)\n",
        "\n",
        "        self.output = np.zeros(self.channels * self.height * self.width).reshape((self.channels,self.height,self.width))\n",
        "\n",
        "        #Install the max value position (max position=1; others =0 )\n",
        "        self.position = np.zeros(self.initial_input_number).reshape(self.inputDimension) #For backpropagation\n",
        "\n",
        "        # will be the inputdimension of next layer\n",
        "        self.dimension = np.shape(self.output)\n",
        "\n",
        "    def get_outputDimension(self):\n",
        "        return self.output.shape\n",
        "\n",
        "    def calculate(self, input): #inputDimension (channels, height, width)\n",
        "\n",
        "        for c in range(self.channels):\n",
        "            for i in range(self.height):\n",
        "                for j in range(self.width):\n",
        "                     matrix =input[c, int(i*self.kernelSize):int((i+1)*self.kernelSize), int(j*self.kernelSize):int((j+1)*self.kernelSize)]\n",
        "                     self.output[c,i,j] = matrix.max()\n",
        "                     #The sequence number of the max number in each kernel\n",
        "                     k=np.argmax(matrix[c])# Will be the number from 0 to kernelSize*kernelSize-1\n",
        "                     #the position of each selected max value can be writen as below\n",
        "                     self.position[c,int((i*self.kernelSize + k) // self.kernelSize), int((j*self.kernelSize + k)% self.kernelSize)] = 1\n",
        "        return self.output\n",
        "\n",
        "    def calcwdeltas(self, wtimesdelta):\n",
        "        expanded_matrix = np.zeros(self.initial_input_number).reshape(self.inputDimension)\n",
        "\n",
        "        for c in range(self.channels):\n",
        "            for i in range(self.height):\n",
        "                for j in range(self.width):\n",
        "                    expanded_matrix[c,int(i*self.kernelSize):int((i+1)*self.kernelSize), int(j*self.kernelSize):int((j+1)*self.kernelSize)]= wtimesdelta[c,i,j]\n",
        "\n",
        "        wtimesdelta = np.zeros(self.initial_input_number).reshape(self.inputDimension)\n",
        "        for c in range(self.inputDimension[0]):\n",
        "            for i in range( self.inputDimension[1]):\n",
        "                for j in range(self.inputDimension[2]):\n",
        "                    wtimesdelta[c,i,j] = expanded_matrix[c,i,j] *self.position[c,i,j]\n",
        "\n",
        "        return wtimesdelta\n",
        "\n",
        "    def print_weights(self, layer_idx):\n",
        "        print('layer {} is a Max Pooling layer without weights'.format(layer_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtGIMX6LuLow"
      },
      "source": [
        "# Flatten Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CS8jFc8huPtL"
      },
      "outputs": [],
      "source": [
        "class FlattenLayer:\n",
        "    def __init__(self,inputDimension):\n",
        "        self.inputDimension = inputDimension\n",
        "        self.output= None\n",
        "\n",
        "    def get_outputDimension(self):\n",
        "        return np.prod(self.inputDimension)\n",
        "\n",
        "    def calculate(self, input):\n",
        "        self.output = input.flatten()\n",
        "        self.output = np.expand_dims(self.output,0)\n",
        "        return self.output\n",
        "\n",
        "    def calcwdeltas(self, wtimesdelta):\n",
        "        wtimesdelta = wtimesdelta.reshape(self.inputDimension)\n",
        "        return wtimesdelta\n",
        "\n",
        "    def print_weights(self, layer_idx):\n",
        "        print('layer {} is a Flatten layer without weights'.format(layer_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSrDdaSduWYy"
      },
      "source": [
        "# Neuron Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wn0GkKW3rCyx"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    # initilize neuron with activation type, number of inputs, learning rate, and possibly with set weights\n",
        "    def __init__(self, activation, input_num, lr, weights=None):\n",
        "        self.activation = activation\n",
        "        self.input_num = input_num\n",
        "        self.lr = lr\n",
        "\n",
        "        # if weights is not set, random set [0,1) in shape of (1, input_num)\n",
        "        if weights is None:\n",
        "            self.weights = np.random.randn(input_num)\n",
        "            self.bias = np.random.randn()\n",
        "        else:\n",
        "            self.weights = weights[:-1]\n",
        "            self.bias = weights[-1]\n",
        "\n",
        "        # some self defined terms\n",
        "        self.output = None\n",
        "        self.input = None\n",
        "        self.delta = None\n",
        "\n",
        "\n",
        "    def print_weight(self, layer_idx, neu_idx):\n",
        "        print('layer {} is a FC layer, weights shape = {} and bias shape = {}. Weights and bias are as follows:'.format(layer_idx, self.weights.shape, self.bias.shape))\n",
        "        print(self.weights)\n",
        "        print(self.bias)\n",
        "\n",
        "    # This method returns the activation of the net\n",
        "    def activate(self, net):\n",
        "        if self.activation == 0:  # linear\n",
        "            output = net\n",
        "        else:  # logistic-sigmoid\n",
        "            output = 1 / (1 + np.exp(-1 * net))\n",
        "        return output\n",
        "\n",
        "    # Calculate the output of the neuron should save the input and output for back-propagation.\n",
        "    def calculate(self, input):\n",
        "        self.input = input\n",
        "        net = np.dot(input, self.weights) + self.bias\n",
        "        self.output = self.activate(net)  # a scalar/ a number\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    # This method returns the derivative of the activation function with respect to the net\n",
        "    def activationderivative(self):\n",
        "        # errorder is the derivative of loss/error with respect to the output\n",
        "        if self.activation == 1:  # logistic\n",
        "            act_der = self.output * (1 - self.output)\n",
        "        else:  # linear\n",
        "            act_der = 1\n",
        "        return act_der  # a scalar\n",
        "\n",
        "    # This method calculates the partial derivative for each weight and returns the delta*w to be used in the previous layer\n",
        "    def calcpartialderivative(self, wtimesdelta):  # wtimesdelta is a matrix\n",
        "        # Be careful about the last hidden layer wtimesdelta should be error_der times actder only\n",
        "        delta = wtimesdelta * self.activationderivative()\n",
        "\n",
        "        self.cal_der_w = delta * self.input  # One node only have one delta value.\n",
        "        self.cal_der_b = delta * 1\n",
        "\n",
        "        # update wtimesdelta\n",
        "        wtimesdelta = delta * self.weights\n",
        "\n",
        "        # update weights\n",
        "        self.updateweight()\n",
        "        return wtimesdelta  # output a vetctor\n",
        "\n",
        "    # Simply update the weights using the partial derivatives and the learning weight\n",
        "    def updateweight(self):\n",
        "        # print('updateweight',self.weights.shape, self.cal_der_w.shape)\n",
        "        self.weights -= self.lr * self.cal_der_w[0]\n",
        "        self.bias -= self.lr * self.cal_der_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqmIGzHuak_"
      },
      "source": [
        "# Fully Connected Layer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BN2o4UpPubeB"
      },
      "outputs": [],
      "source": [
        "# A fully connected layer\n",
        "class FullyConnected:\n",
        "    # initialize with the number of neurons in the layer, their activation,the input size,\n",
        "    # the learning rate and a 2d matrix of weights (or else initilize randomly)\n",
        "    def __init__(self, numOfNeurons, activation, input_num, lr, weights=None):\n",
        "        self.numOfNeurons = numOfNeurons\n",
        "        self.neurons = []  # neurons works as a new list containing all Neurons\n",
        "        self.output_shape = numOfNeurons\n",
        "        if weights is None:\n",
        "            for i in range(numOfNeurons):\n",
        "                self.neurons.append(Neuron(activation, input_num, lr))\n",
        "        else:\n",
        "            for i in range(numOfNeurons):\n",
        "                self.neurons.append(Neuron(activation, input_num, lr, weights))\n",
        "\n",
        "    def get_outputDimension(self):\n",
        "        return (self.numOfNeurons, 1)\n",
        "\n",
        "    def print_weights(self, layer_idx):\n",
        "        for neuron_index in range(self.numOfNeurons):\n",
        "            self.neurons[neuron_index].print_weight(layer_idx, neuron_index)\n",
        "\n",
        "    # calcualte the output of all the neurons in the layer and return a vector with those values (go through the neurons and call the calcualte() method)\n",
        "    def calculate(self, input):  # input:(2,)\n",
        "        self.output_vector = []  # list\n",
        "        for i in range(self.numOfNeurons):\n",
        "            self.output_vector.append(self.neurons[i].calculate(input))\n",
        "        return np.array(self.output_vector)  # array\n",
        "\n",
        "    # given the next layer's w*delta, should run through the neurons calling calcpartialderivative() for each (with the correct value),\n",
        "    # sum up its ownw*delta, and then update the wieghts (using the updateweight() method).\n",
        "    # should return the sum of w*delta. - A vector\n",
        "    def calcwdeltas(self, wtimesdelta):\n",
        "        lst = []\n",
        "\n",
        "        for i in range(self.numOfNeurons):\n",
        "            lst.append(self.neurons[i].calcpartialderivative(wtimesdelta[i]))\n",
        "        lst = np.array(lst)\n",
        "\n",
        "        wtimesdelta = np.sum(lst, axis=0)\n",
        "        return wtimesdelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq7ZWCexulIP"
      },
      "source": [
        "# Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yposRlO5ui1W"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    # Initialize the instance instance variables 'inputSize', 'activation', 'loss', 'lr' and 'weights' \n",
        "    def __init__(self, inputSize, activation, loss, lr, weights=None):\n",
        "        self.loss = loss # Set loss function\n",
        "        self.activation = activation # Set activation function\n",
        "        self.lr = lr # Set learning rate\n",
        "        self.layers = [] # Initialize list of layers\n",
        "        self.inputSize = inputSize # Set input size\n",
        "\n",
        "\n",
        "    # Add a new layer to the neural network and it takes two arguments, 'type' and 'layer_param'\n",
        "    def addLayer(self,type,layer_param):\n",
        "\n",
        "        # If this is the first layer\n",
        "        if self.layers == []: \n",
        "            input_shape = self.inputSize # Set input shape to input size\n",
        "        else:\n",
        "            input_shape = self.layers[-1].get_outputDimension() # Otherwise, set input shape to output shape of previous layer\n",
        "\n",
        "        # If the layer type is convolutional\n",
        "        if type == 'conv': \n",
        "            self.layers.append(ConvolutionalLayer(layer_param['numOfKernels'], layer_param['kernelSize'],layer_param['activation']\n",
        "                                                  ,input_shape, self.lr, layer_param['weights'],layer_param['bias'])) # Add a convolutional layer to the list of layers\n",
        "        \n",
        "        # If layer type is max pooling\n",
        "        elif type == 'max': \n",
        "            self.layers.append(MaxPoolingLayer(layer_param['poolingSize'], input_shape)) # Add a max pooling layer to the list of layers\n",
        "\n",
        "        # If layer type is flatten\n",
        "        elif type == 'fl': \n",
        "            self.layers.append(FlattenLayer(input_shape)) # Add a flattern layer to the list of layers\n",
        "\n",
        "        # Otherwise, assume fully connected layer    \n",
        "        else: \n",
        "            self.layers.append(FullyConnected(layer_param['numOfNeurons'], layer_param['activation'], input_shape, self.lr,\n",
        "                                                      layer_param['weights']))\n",
        "\n",
        "    # Feed the input data through the network and obtain the output of the last layer\n",
        "    def calculate(self, input):\n",
        "        for j in range(input.shape[0]):\n",
        "            tmp_output = input[j] # Initialize output to current input\n",
        "\n",
        "            # For each layer in the list of layers, calculate the output of the layer\n",
        "            for i in range(len(self.layers)):\n",
        "                tmp_output = self.layers[i].calculate(tmp_output)\n",
        "\n",
        "        return tmp_output\n",
        "\n",
        "\n",
        "    # Return the calculated loss\n",
        "    def calculateloss(self, yp, y):  \n",
        "        error = 0 # Initial error\n",
        "\n",
        "        # For each prediction in the prediction array \n",
        "        for i in range(yp.shape[0]):\n",
        "            if self.loss == 0: # if the loss function is mean squared error\n",
        "                error += 1 / 2 * np.sum((yp[i] - y[i]) ** 2)\n",
        "            else:  # Otherwise, assume cross-entropy loss function\n",
        "                error += np.sum(np.nan_to_num(-y[i] * np.log(yp[i]) - (1 - y[i]) * np.log(1 - yp[i])))\n",
        "\n",
        "        return error / yp.shape[0] # Return the average error over all predictions \n",
        "    \n",
        "    # Compute the derivative of the loss with respect to the final output of the neural network, 'yp', given the true target values 'y'. \n",
        "    def lossderiv(self, yp, y):\n",
        "\n",
        "        if self.loss == 0: # if loss function is mean squared error\n",
        "            error_der = 0\n",
        "            for i in range(yp.shape[0]):\n",
        "                error_der += (yp[i] - y[i])\n",
        "        else: # Otherwise, assume cross-entropy loss function\n",
        "\n",
        "            if self.activation == 0:  # Add softmax\n",
        "                error_der = 0\n",
        "                for i in range(yp.shape[0]):\n",
        "                    error_der += ((yp[i] - y[i]) / yp[i] / (1 - yp[i])) * yp[i]  \n",
        "            else:\n",
        "                error_der = 0\n",
        "                for i in range(yp.shape[0]):\n",
        "                    error_der = np.nan_to_num(((yp[i] - y[i]) / yp[i] / (1 - yp[i])))  \n",
        "\n",
        "        return (1 / yp.shape[0]) * error_der\n",
        "\n",
        "    # Train the neural network model \n",
        "    def train(self, x, y, num_epochs=1000):\n",
        "        error = []\n",
        "        for epoch_index in range(num_epochs):\n",
        "            output_last = self.calculate(x)\n",
        "            yp = output_last\n",
        "            if error == []:\n",
        "                past_loss = 1e+10\n",
        "            else:\n",
        "                past_loss = current_loss\n",
        "\n",
        "            current_loss = self.calculateloss(yp, y)\n",
        "\n",
        "            if np.abs(current_loss - past_loss) <= 1e-8:\n",
        "               break\n",
        "\n",
        "\n",
        "            error.append(current_loss)  \n",
        "            error_der = self.lossderiv(yp, y)* 2\n",
        "            wtimesdelta = error_der  \n",
        "            for layer_index in range(len(self.layers)-1, -1, -1):  \n",
        "                wtimesdelta = self.layers[layer_index].calcwdeltas(wtimesdelta)\n",
        "\n",
        "        return yp, error[-1]\n",
        "\n",
        "\n",
        "    def print_weights(self):\n",
        "        for layer_index in range(len(self.layers)):\n",
        "            self.layers[layer_index].print_weights(layer_index)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verifying Results with Keras\n",
        "In this section, we will verify the results of our implementation with Keras."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example1():\n",
        "    \n",
        "    # set random seed\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # create input and output for keras model\n",
        "    keras_input = np.random.rand(1, 5, 5, 1)\n",
        "    keras_output = np.random.rand(1)\n",
        "\n",
        "    our_input = keras_input.reshape(1, 5, 5)\n",
        "    our_label = [keras_output]\n",
        "\n",
        "    keras_conv_w = np.random.rand(3, 3, 1, 1)\n",
        "    keras_conv_b = np.random.rand(1)\n",
        "    keras_fc_w = np.random.rand(9, 1)\n",
        "    keras_fc_b = np.random.rand(1)\n",
        "\n",
        "    # create input and output for our model\n",
        "    our_conv_w = keras_conv_w.reshape(1, 1, 3, 3)\n",
        "    our_conv_b = keras_conv_b.reshape(1)\n",
        "    our_fc_w = keras_fc_w.reshape(9)\n",
        "    our_fc_b = keras_fc_b.reshape(1)\n",
        "\n",
        "    our_fc_w = np.hstack((our_fc_w, our_fc_b))\n",
        "\n",
        "    return keras_input,keras_output,keras_conv_w, keras_conv_b, keras_fc_w, keras_fc_b, \\\n",
        "           our_input,our_label,our_conv_w, our_conv_b,our_fc_w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output before backprop: [0.63992102]\n",
            "----- Keras Model after Backpropagation (Example 1)-----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-06 15:18:00.871089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 194ms/step - loss: 0.1269 - accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "Output from Keras model after backpropagation is [[0.94939]]\n",
            "1st conv layer weight is \n",
            "[[[[ 0.10745  0.91042  0.49045]\n",
            "   [ 0.38067  0.23018  0.73869]\n",
            "   [ 0.42771  0.53635 -0.01125]]]]\n",
            "1st conv layer weight is \n",
            "[0.56307]\n",
            "FC layer weight is \n",
            "[ 0.34338  0.34923  0.67778  0.42426  0.08943  0.1702   0.43194 -0.20615\n",
            "  0.39757]\n",
            "FC layer bias is \n",
            "[0.39325]\n"
          ]
        }
      ],
      "source": [
        "keras_input, keras_output, keras_conv_w, keras_conv_b, keras_fc_w, keras_fc_b, \\\n",
        "our_input, our_label, our_conv_w, our_conv_b, our_fc_w=example1()\n",
        "\n",
        "input = keras_input\n",
        "output = keras_output\n",
        "print('Output before backprop:', output)\n",
        "\n",
        "print('----- Keras Model after Backpropagation (Example 1)-----')\n",
        "model = Sequential()\n",
        "model.add(layers.Conv2D(1, 3, input_shape=(5, 5, 1), activation='sigmoid'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].set_weights([keras_conv_w, keras_conv_b])\n",
        "model.layers[2].set_weights([keras_fc_w,keras_fc_b])\n",
        "\n",
        "np.set_printoptions(precision=5)\n",
        "sgd = optimizers.SGD(learning_rate=100) #change suggested by Prof Amir\n",
        "model.compile(loss='MSE', optimizer=sgd, metrics=['accuracy'])\n",
        "history = model.fit(input, output, batch_size=1, epochs=1)\n",
        "print('Output from Keras model after backpropagation is', model.predict(input))\n",
        "\n",
        "print('1st conv layer weight is ')\n",
        "print(model.get_weights()[0].reshape(1, 1, 3, 3))\n",
        "print('1st conv layer weight is ')\n",
        "print(model.get_weights()[1])\n",
        "print('FC layer weight is ')\n",
        "print(model.get_weights()[2].reshape(9))\n",
        "print('FC layer bias is ')\n",
        "print(model.get_weights()[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Our Model after Backpropagation (Example 1) -----\n",
            "Output from our model after backpropagation is: [[0.94972]]\n",
            "layer 0 is a Conv layer, weights shape = (1, 1, 3, 3) and bias shape = (1,). Weights and bias are as follows:\n",
            "[[[[ 0.10745  0.91042  0.49045]\n",
            "   [ 0.38067  0.23018  0.73869]\n",
            "   [ 0.42771  0.53635 -0.01125]]]]\n",
            "[0.56307]\n",
            "layer 1 is a Flatten layer without weights\n",
            "layer 2 is a FC layer, weights shape = (9,) and bias shape = (1,). Weights and bias are as follows:\n",
            "[ 0.34338  0.34923  0.67778  0.42426  0.08943  0.1702   0.43194 -0.20615\n",
            "  0.39757]\n",
            "[0.39325]\n"
          ]
        }
      ],
      "source": [
        "input = our_input\n",
        "label = our_label\n",
        "\n",
        "conv_layer_param = {'numOfKernels':1, 'kernelSize':3, 'activation':'sigmoid', 'weights': our_conv_w, 'bias':our_conv_b}\n",
        "fl_layer_param = {}\n",
        "fc_layer_param = {'numOfNeurons': 1, 'activation': 1,'weights': our_fc_w}\n",
        "\n",
        "print('----- Our Model after Backpropagation (Example 1) -----')\n",
        "\n",
        "model = NeuralNetwork(input.shape, activation='sigmoid', loss=0, lr=100)\n",
        "model.addLayer('conv', conv_layer_param)\n",
        "model.addLayer('fl', fl_layer_param)\n",
        "model.addLayer('fc', fc_layer_param)\n",
        "yp, error = model.train(np.array([input]), np.array(label), num_epochs=1)\n",
        "print(f\"Output from our model after backpropagation is: {model.calculate(np.array([input]))}\")\n",
        "model.print_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example2():\n",
        "    # set random seed\n",
        "    np.random.seed(0)\n",
        "\n",
        "\n",
        "    # create input and output for keras model\n",
        "    keras_input = np.random.rand(1, 7, 7, 1)\n",
        "    keras_output = np.random.rand(1)\n",
        "\n",
        "    our_input = keras_input.reshape(1, 7, 7)\n",
        "    our_label = [keras_output]\n",
        "\n",
        "    keras_conv1_w = np.random.rand(3, 3, 1, 2)\n",
        "    keras_conv1_b = np.random.rand(2)\n",
        "    keras_conv2_w = np.random.rand(3, 3, 2, 1)\n",
        "    keras_conv2_b = np.random.rand(1)\n",
        "    keras_fc_w = np.random.rand(9, 1)\n",
        "    keras_fc_b = np.random.rand(1)\n",
        "\n",
        "    # create input and output for our model\n",
        "    our_conv1_w = keras_conv1_w.reshape(2, 1, 3, 3)\n",
        "    our_conv1_b = keras_conv1_b.reshape(2)\n",
        "    our_conv2_w = keras_conv2_w.reshape(1, 2, 3, 3)\n",
        "    our_conv2_b = keras_conv2_b.reshape(1)\n",
        "    our_fc_w = keras_fc_w.reshape(9)\n",
        "    our_fc_b = keras_fc_b.reshape(1)\n",
        "\n",
        "    our_fc_w = np.hstack((our_fc_w, our_fc_b))\n",
        "\n",
        "    return keras_input,keras_output,keras_conv1_w, keras_conv1_b, keras_conv2_w, keras_conv2_b, keras_fc_w, keras_fc_b, \\\n",
        "           our_input,our_label,our_conv1_w, our_conv1_b,our_conv2_w, our_conv2_b,our_fc_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras_input, keras_output, keras_conv1_w, keras_conv1_b, keras_conv2_w, keras_conv2_b, keras_fc_w, keras_fc_b, \\\n",
        "our_input, our_label, our_conv1_w, our_conv1_b, our_conv2_w, our_conv2_b, our_fc_w = example2()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label: [0.36371]\n",
            "-----Keras Model after Backpropagation (Example 2)-----\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.3931 - accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "keras model output after backprop is [[0.00104]]\n",
            "1st conv layer weight is \n",
            "[[[[0.56994 0.43787 0.98811]\n",
            "   [0.10116 0.20855 0.16035]\n",
            "   [0.65284 0.25239 0.46602]]]\n",
            "\n",
            "\n",
            " [[[0.24351 0.15868 0.10935]\n",
            "   [0.65611 0.1374  0.19631]\n",
            "   [0.36792 0.82073 0.09616]]]]\n",
            "1st conv layer bias is \n",
            "[0.83742 0.09441]\n",
            "2nd conv layer weight is \n",
            "[[[[0.97344 0.46631 0.9737 ]\n",
            "   [0.60247 0.73619 0.03673]\n",
            "   [0.27976 0.11789 0.29311]]\n",
            "\n",
            "  [[0.11632 0.3149  0.41187]\n",
            "   [0.06113 0.69016 0.56356]\n",
            "   [0.26302 0.52019 0.09156]]]]\n",
            "2nd conv layer bias is \n",
            "[0.57278]\n",
            "FC layer weight is \n",
            "[-0.2247  -0.83551 -0.48674 -1.02226 -0.43772 -0.86473 -0.97078 -0.56753\n",
            " -1.13398]\n",
            "FC layer bias is \n",
            "[-0.32594]\n"
          ]
        }
      ],
      "source": [
        "input = keras_input\n",
        "output = keras_output\n",
        "print('label:', output)\n",
        "\n",
        "print('-----Keras Model after Backpropagation (Example 2)-----')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(2, 3, input_shape=(7, 7, 1), activation='sigmoid'))\n",
        "model.add(layers.Conv2D(1, 3, activation='sigmoid'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].set_weights([keras_conv1_w, keras_conv1_b])\n",
        "model.layers[1].set_weights([keras_conv2_w,keras_conv2_b])\n",
        "model.layers[3].set_weights([keras_fc_w, keras_fc_b])\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=5)\n",
        "sgd = optimizers.SGD(learning_rate=100)\n",
        "model.compile(loss='MSE', optimizer=sgd, metrics=['accuracy'])\n",
        "history = model.fit(input, output, batch_size=1, epochs=1)\n",
        "print('keras model output after backprop is', model.predict(input))\n",
        "\n",
        "print('1st conv layer weight is ')\n",
        "print(model.get_weights()[0].reshape(2, 1, 3, 3))\n",
        "print('1st conv layer bias is ')\n",
        "print(model.get_weights()[1])\n",
        "print('2nd conv layer weight is ')\n",
        "print(model.get_weights()[2].reshape(1, 2, 3, 3))\n",
        "print('2nd conv layer bias is ')\n",
        "print(model.get_weights()[3])\n",
        "print('FC layer weight is ')\n",
        "print(model.get_weights()[4].reshape(9))\n",
        "print('FC layer bias is ')\n",
        "print(model.get_weights()[5])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Our Model after Backpropagation (Example 2) -----\n",
            "Output from our model after backpropagation is: [[0.00104]]\n",
            "layer 0 is a Conv layer, weights shape = (2, 1, 3, 3) and bias shape = (2,). Weights and bias are as follows:\n",
            "[[[[0.56992 0.43826 0.98801]\n",
            "   [0.1017  0.20854 0.16094]\n",
            "   [0.65281 0.25293 0.46601]]]\n",
            "\n",
            "\n",
            " [[[0.24378 0.15826 0.10963]\n",
            "   [0.65578 0.13753 0.19584]\n",
            "   [0.3681  0.82037 0.09638]]]]\n",
            "[0.83607 0.09422]\n",
            "layer 1 is a Conv layer, weights shape = (1, 2, 3, 3) and bias shape = (1,). Weights and bias are as follows:\n",
            "[[[[0.97157 0.46366 0.97167]\n",
            "   [0.59993 0.73423 0.03417]\n",
            "   [0.27788 0.11521 0.29107]]\n",
            "\n",
            "  [[0.11384 0.31299 0.40918]\n",
            "   [0.05923 0.68744 0.56159]\n",
            "   [0.26046 0.51826 0.08887]]]]\n",
            "[0.57312]\n",
            "layer 2 is a Flatten layer without weights\n",
            "layer 3 is a FC layer, weights shape = (9,) and bias shape = (1,). Weights and bias are as follows:\n",
            "[-0.22442 -0.83527 -0.48649 -1.02196 -0.43751 -0.86449 -0.97057 -0.56737\n",
            " -1.13374]\n",
            "[-0.32562]\n"
          ]
        }
      ],
      "source": [
        "print('----- Our Model after Backpropagation (Example 2) -----')\n",
        "input = our_input\n",
        "label = our_label\n",
        "\n",
        "conv1_layer_param = {'numOfKernels': 2, 'kernelSize': 3, 'activation': 'sigmoid', 'weights': our_conv1_w, 'bias': our_conv1_b}\n",
        "conv2_layer_param = {'numOfKernels': 1, 'kernelSize': 3, 'activation': 'sigmoid', 'weights': our_conv2_w, 'bias': our_conv2_b}\n",
        "fl_layer_param = {}\n",
        "fc_layer_param = {'numOfNeurons': 1, 'activation': 1, 'weights': our_fc_w}\n",
        "\n",
        "model = NeuralNetwork(input.shape, activation='sigmoid', loss=0, lr=100)\n",
        "model.addLayer('conv', conv1_layer_param)\n",
        "model.addLayer('conv', conv2_layer_param)\n",
        "model.addLayer('fl', fl_layer_param)\n",
        "model.addLayer('fc', fc_layer_param)\n",
        "yp, error = model.train(np.array([input]), np.array(label), num_epochs=1)\n",
        "print(f\"Output from our model after backpropagation is: {model.calculate(np.array([input]))}\")\n",
        "model.print_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def example3():\n",
        "    # set random seed\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # create input and output for keras model\n",
        "    keras_input = np.random.rand(1, 8, 8, 1)\n",
        "    keras_output = np.random.rand(1)\n",
        "\n",
        "    our_input = keras_input.reshape(1, 8, 8)\n",
        "    our_label = [keras_output]\n",
        "\n",
        "    keras_conv_w = np.random.rand(3, 3, 1, 2)\n",
        "    keras_conv_b = np.random.rand(2)\n",
        "    keras_fc_w = np.random.rand(18, 1)\n",
        "    keras_fc_b = np.random.rand(1)\n",
        "\n",
        "    # create input and output for our model\n",
        "    our_conv_w = keras_conv_w.reshape(2, 1, 3, 3)\n",
        "    our_conv_b = keras_conv_b.reshape(2)\n",
        "    our_fc_w = keras_fc_w.reshape(18)\n",
        "    our_fc_b = keras_fc_b.reshape(1)\n",
        "\n",
        "    our_fc_w = np.hstack((our_fc_w, our_fc_b))\n",
        "    return keras_input,keras_output,keras_conv_w, keras_conv_b, keras_fc_w, keras_fc_b, \\\n",
        "           our_input,our_label,our_conv_w, our_conv_b,our_fc_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras_input, keras_output, keras_conv_w, keras_conv_b, keras_fc_w, keras_fc_b, \\\n",
        "        our_input, our_label, our_conv_w, our_conv_b, our_fc_w=example3()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label: [0.19658]\n",
            "-----Keras Model after Backpropagation (Example 3)-----\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.6451 - accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Keras model output after backprop is [[0.99954]]\n",
            "1st conv layer weight is \n",
            "[[[[0.3613  0.81864 0.08867]\n",
            "   [0.83532 0.08839 0.97368]\n",
            "   [0.46092 0.9748  0.59834]]]\n",
            "\n",
            "\n",
            " [[[0.73691 0.03401 0.28045]\n",
            "   [0.11179 0.29337 0.11395]\n",
            "   [0.31612 0.40792 0.06221]]]]\n",
            "1st conv layer bias is \n",
            "[0.68129 0.56232]\n",
            "FC layer weight is \n",
            "[ 0.23038  0.48557  0.05873  0.53793  0.8948   0.28099  0.63399  0.09414\n",
            "  0.68126  0.25137  0.14858  0.54896 -0.01321  0.79247 -0.02798  0.6407\n",
            "  0.23616  0.69747]\n",
            "FC layer bias is \n",
            "[0.92381]\n"
          ]
        }
      ],
      "source": [
        "input = keras_input\n",
        "output = keras_output\n",
        "print('label:', output)\n",
        "\n",
        "print('-----Keras Model after Backpropagation (Example 3)-----')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(2, 3, input_shape=(8, 8, 1), activation='sigmoid'))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].set_weights([keras_conv_w, keras_conv_b])\n",
        "model.layers[3].set_weights([keras_fc_w,keras_fc_b])\n",
        "\n",
        "np.set_printoptions(precision=5)\n",
        "\n",
        "\n",
        "sgd = optimizers.SGD(learning_rate=100)\n",
        "model.compile(loss='MSE', optimizer=sgd, metrics=['accuracy'])\n",
        "history = model.fit(input, output, batch_size=1, epochs=1)\n",
        "print('Keras model output after backprop is',model.predict(input))\n",
        "\n",
        "print('1st conv layer weight is ')\n",
        "print(model.get_weights()[0].reshape(2, 1, 3, 3))\n",
        "print('1st conv layer bias is ')\n",
        "print(model.get_weights()[1])\n",
        "print('FC layer weight is ')\n",
        "print(model.get_weights()[2].reshape(18))\n",
        "print('FC layer bias is ')\n",
        "print(model.get_weights()[3])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Our Model after Backpropagation (Example 3) -----\n",
            "Output from our model after backpropagation is: [[0.99949]]\n",
            "layer 0 is a Conv layer, weights shape = (2, 1, 3, 3) and bias shape = (2,). Weights and bias are as follows:\n",
            "[[[[0.36844 0.81934 0.09561]\n",
            "   [0.83743 0.09477 0.9759 ]\n",
            "   [0.46767 0.97559 0.60441]]]\n",
            "\n",
            "\n",
            " [[[0.73612 0.03587 0.27964]\n",
            "   [0.11721 0.2927  0.11492]\n",
            "   [0.31595 0.41138 0.06064]]]]\n",
            "[0.68531 0.55944]\n",
            "layer 1 is a Max Pooling layer without weights\n",
            "layer 2 is a Flatten layer without weights\n",
            "layer 3 is a FC layer, weights shape = (18,) and bias shape = (1,). Weights and bias are as follows:\n",
            "[ 0.22502  0.48283  0.05397  0.53595  0.88895  0.27833  0.62816  0.09185\n",
            "  0.67719  0.25242  0.14616  0.55032 -0.01551  0.79188 -0.03224  0.64272\n",
            "  0.23377  0.69929]\n",
            "[0.92134]\n"
          ]
        }
      ],
      "source": [
        "print('----- Our Model after Backpropagation (Example 3) -----')\n",
        "input = our_input\n",
        "label = our_label\n",
        "\n",
        "conv_layer_param = {'numOfKernels': 2, 'kernelSize': 3, 'activation': 'sigmoid', 'weights': our_conv_w, 'bias': our_conv_b}\n",
        "max_layer_param = {'poolingSize': 2}\n",
        "fl_layer_param = {}\n",
        "fc_layer_param = {'numOfNeurons': 1, 'activation': 1, 'weights': our_fc_w}\n",
        "\n",
        "model = NeuralNetwork(input.shape, activation='sigmoid', loss=0, lr=100)\n",
        "model.addLayer('conv', conv_layer_param)\n",
        "model.addLayer('max', max_layer_param)\n",
        "model.addLayer('fl', fl_layer_param)\n",
        "model.addLayer('fc', fc_layer_param)\n",
        "yp, error = model.train(np.array([input]), np.array(label), num_epochs=1)\n",
        "print(f\"Output from our model after backpropagation is: {model.calculate(np.array([input]))}\")\n",
        "model.print_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
